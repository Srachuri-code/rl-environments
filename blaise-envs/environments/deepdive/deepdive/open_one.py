import asyncio
import concurrent.futures
import functools
import io
import os
import re
import threading
from time import perf_counter
from urllib.parse import urljoin, urlparse

import aiohttp
from diskcache import Cache

# === Thread pool configuration ===
_thread_pool: concurrent.futures.ThreadPoolExecutor | None = None
_max_workers: int = 64  # default
_pool_lock = threading.Lock()


def configure_thread_pool(max_workers: int = 64) -> None:
    """Configure the thread pool. Call before first use."""
    global _thread_pool, _max_workers
    with _pool_lock:
        _max_workers = max_workers
        if _thread_pool is not None:
            _thread_pool.shutdown(wait=False)
        _thread_pool = None  # Will be lazily created


def _get_thread_pool() -> concurrent.futures.ThreadPoolExecutor:
    global _thread_pool
    if _thread_pool is None:
        with _pool_lock:
            # Double-check after acquiring lock
            if _thread_pool is None:
                _thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=_max_workers)
    return _thread_pool


async def run_in_executor(func, *args, **kwargs):
    """
    Run a CPU-bound or C-extension-heavy function in the thread pool.
    Keeps the call sites simple and avoids threading issues in libraries.
    """
    loop = asyncio.get_running_loop()
    if kwargs:
        func = functools.partial(func, **kwargs)
    return await loop.run_in_executor(_get_thread_pool(), func, *args)


# === Disk cache for cross-process sharing ===
# Set DEEPDIVE_CACHE_DIR to a shared filesystem path for multi-node deployments
_cache_dir: str = os.environ.get("DEEPDIVE_CACHE_DIR", "/tmp/deepdive_cache")
_cache_size_limit: int = 10 * 1024**3  # 10GB default
_cache_ttl: int = 604800  # 1 week default
_disk_cache: Cache | None = None
_cache_lock = threading.Lock()


def configure_cache(
    cache_dir: str | None = None,
    size_limit_gb: int = 10,
    ttl_seconds: int = 604800,
) -> None:
    """Configure the disk cache. Call before first use."""
    global _disk_cache, _cache_dir, _cache_size_limit, _cache_ttl
    with _cache_lock:
        if cache_dir is not None:
            _cache_dir = cache_dir
        _cache_size_limit = size_limit_gb * 1024**3
        _cache_ttl = ttl_seconds
        if _disk_cache is not None:
            _disk_cache.close()
        _disk_cache = None  # Will be lazily created


def get_disk_cache() -> Cache:
    global _disk_cache
    if _disk_cache is None:
        with _cache_lock:
            if _disk_cache is None:
                _disk_cache = Cache(
                    _cache_dir,
                    size_limit=_cache_size_limit,
                    timeout=300,  # 5 minute SQLite busy timeout for high concurrency
                )
    return _disk_cache


def close_cache() -> None:
    """Properly close the disk cache.

    Call this on shutdown to ensure SQLite is properly closed.
    """
    global _disk_cache
    with _cache_lock:
        if _disk_cache is not None:
            _disk_cache.close()
            _disk_cache = None


# === In-process single-flight ===
_inflight: dict[str, asyncio.Future] = {}
_fetch_semaphore = asyncio.Semaphore(64)


def truncate_text(text: str, max_length: int) -> str:
    """Truncate a large text blob with a clear sentinel."""
    if len(text) > max_length:
        return text[:max_length] + "\n...\n[truncated]"
    return text


def looks_like_pdf(url: str, headers: dict, body: bytes) -> bool:
    """Heuristic PDF detector based on headers and magic bytes."""
    content_type = headers.get("Content-Type") or headers.get("content-type") or ""
    content_disposition = headers.get("Content-Disposition") or headers.get("content-disposition") or ""
    ct = content_type.lower()
    disp = content_disposition.lower()
    path = urlparse(url).path.lower()

    stripped = body.lstrip(b"\r\n\t\f\x00")
    header_is_pdf = stripped.startswith(b"%PDF-")

    return (
        "application/pdf" in ct
        or "application/x-pdf" in ct
        or ("application/octet-stream" in ct and header_is_pdf)
        or path.endswith(".pdf")
        or ("filename=" in disp and ".pdf" in disp)
        or header_is_pdf
    )


def pdf_to_text_bytes(pdf_bytes):
    import logging

    from pdfminer.high_level import extract_text

    pm_logger = logging.getLogger("pdfminer")
    pm_logger.setLevel(logging.ERROR)

    with io.BytesIO(pdf_bytes) as f:
        return extract_text(f)


async def clean_text_to_markdown(text):
    text = re.sub(r"[ \t]+\n", "\n", text)
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"(\w+)-\n(\w+)", r"\1\2", text)
    text = text.replace("\f", "\n\n---\n\n")
    return text.strip()


async def fetch_llm_readable(url, timeout=30, headers=None, debug=False):
    """Fetch and extract readable content from a URL."""
    headers = headers or {"User-Agent": "Mozilla/5.0"}
    timeout_obj = aiohttp.ClientTimeout(total=timeout)

    async with aiohttp.ClientSession(timeout=timeout_obj, headers=headers) as session:
        t0 = perf_counter()
        try:
            async with session.get(url) as r:
                try:
                    r.raise_for_status()
                except Exception as e:
                    if debug:
                        print(f"Error fetching {url}: {e}")
                    return {
                        "type": "error",
                        "content": f"Error fetching {url}: {e}",
                        "source": url,
                        "format": "error",
                    }

                if debug:
                    print(f"Fetched {url} in {perf_counter() - t0:.2f}s")

                raw_bytes = await r.read()
                headers_lower = {k: v for k, v in r.headers.items()}
                content_type = headers_lower.get("Content-Type", "").lower()

                is_pdf = looks_like_pdf(url, headers_lower, raw_bytes)
                if is_pdf:
                    try:
                        text = await run_in_executor(pdf_to_text_bytes, raw_bytes)
                        md = await clean_text_to_markdown(text or "")
                        result = {
                            "type": "markdown",
                            "content": md,
                            "source": url,
                            "format": "pdf->text(pdfminer)",
                        }
                    except Exception:
                        is_pdf = False

                if not is_pdf:
                    encoding = r.charset or "utf-8"
                    try:
                        html_text = raw_bytes.decode(encoding, errors="ignore")
                    except LookupError:
                        html_text = raw_bytes.decode("utf-8", errors="ignore")

                    if "text/html" in content_type or "<html" in html_text.lower():
                        m = re.search(
                            r'(?:<embed|<iframe)[^>]+src=["\']([^"\']+\.pdf)[^"\']*["\']',
                            html_text,
                            re.I,
                        )
                        if m:
                            pdf_url = urljoin(url, m.group(1))
                            return await fetch_llm_readable(pdf_url, timeout, headers, debug)

                    import trafilatura

                    md = await run_in_executor(
                        trafilatura.extract,
                        html_text,
                        output_format="markdown",
                        fast=True,
                    )
                    result = {
                        "type": "markdown",
                        "content": md or "",
                        "source": url,
                        "format": "html->trafilatura",
                    }

                if debug:
                    print(f"Rendered {url} in {perf_counter() - t0:.2f}s")
        except Exception as e:
            if debug:
                print(f"Error rendering {url}: {e}")
            result = {
                "type": "error",
                "content": f"Error rendering {url}: {e}",
                "source": url,
                "format": "error",
            }
        return result


async def _do_fetch_and_parse(url: str, debug: bool = False) -> str:
    async with _fetch_semaphore:
        result = await fetch_llm_readable(url, debug=debug)
        return result["content"]


async def open_one(url: str, debug: bool = False) -> str:
    """
    Fetch URL content with two-layer single-flight deduplication:
    - Layer 1 (in-process): asyncio.Future for instant notification
    - Layer 2 (cross-process): diskcache Lock + shared cache
    """
    t0 = perf_counter()
    cache = get_disk_cache()

    # 1. Check disk cache first (cross-process)
    cached = await asyncio.to_thread(cache.get, url, retry=True)
    if cached is not None:
        if debug:
            print(f"[disk cache hit] {url} in {perf_counter() - t0:.2f}s")
        return cached

    # 2. In-process single-flight: check if another coroutine is already fetching
    flight = _inflight.get(url)
    if flight is not None:
        if debug:
            print(f"[awaiting local future] {url}")
        return await flight

    # 3. We're the first in this process - create Future for others to await
    fut: asyncio.Future = asyncio.get_event_loop().create_future()
    _inflight[url] = fut

    try:
        content = await _fetch_with_cross_process_coordination(url, cache, debug)
        fut.set_result(content)
        if debug:
            print(f"[completed] {url} in {perf_counter() - t0:.2f}s")
        return content
    except BaseException as e:  # Catch CancelledError too (inherits from BaseException)
        fut.set_exception(e)
        raise
    finally:
        _inflight.pop(url, None)


async def _fetch_with_cross_process_coordination(url: str, cache: Cache, debug: bool) -> str:
    """Handle cross-process single-flight via atomic cache operations."""
    lock_key = f"lock:{url}"
    deadline = perf_counter() + 120  # Single deadline for entire operation

    while True:
        # Try to acquire lock atomically - add() returns True only if key didn't exist
        acquired = await asyncio.to_thread(cache.add, lock_key, "locked", expire=120, retry=True)

        if acquired:
            try:
                # Double-check cache (another process might have just finished)
                cached = await asyncio.to_thread(cache.get, url, retry=True)
                if cached is not None:
                    return cached

                # We're the fetcher
                content = await _do_fetch_and_parse(url, debug=debug)
                content = truncate_text(content, 20_000)
                await asyncio.to_thread(cache.set, url, content, expire=_cache_ttl, retry=True)
                return content
            finally:
                # Release lock
                await asyncio.to_thread(cache.delete, lock_key, retry=True)
        else:
            # Another process is fetching - poll until result appears
            # (Only THIS coroutine polls; others in this process await our Future)
            if debug:
                print(f"[waiting for peer process] {url}")

            backoff = 0.05

            while perf_counter() < deadline:
                await asyncio.sleep(backoff)
                backoff = min(backoff * 1.3, 0.25)  # Cap at 0.25s for faster response

                cached = await asyncio.to_thread(cache.get, url, retry=True)
                if cached is not None:
                    return cached

                # Check if lock holder died (lock expired)
                lock_exists = await asyncio.to_thread(cache.get, lock_key, retry=True)
                if lock_exists is None:
                    # Lock released - retry acquiring (continue outer loop)
                    break
            else:
                # Deadline exceeded - fetch ourselves as last resort
                if debug:
                    print(f"[timeout, fetching anyway] {url}")
                content = await _do_fetch_and_parse(url, debug=debug)
                content = truncate_text(content, 20_000)
                await asyncio.to_thread(cache.set, url, content, expire=_cache_ttl, retry=True)
                return content
